# -*- coding: utf-8 -*-
"""SmartNewsRadar - æ™ºèƒ½æ–°é—»å…³é”®è¯å‘ç°ç³»ç»Ÿ
åŸºäºAIé©±åŠ¨çš„çœŸå®ä¸–ç•Œçƒ­ç‚¹å…³é”®è¯è‡ªåŠ¨å‘ç°
"""

import asyncio
import json
import os
import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Set, Optional

import jieba
import jieba.analyse
import yaml
from dataclasses import dataclass, field

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from enhanced_data_fetcher import NewsFetcher
from smart_learning import SmartLearningEngine  
from smart_presentation import present_results

# é…ç½®æ—¥å¿— - æç®€é«˜æ•ˆ
import logging
# ç¡®ä¿è¿™æ˜¯ç¨‹åºä¸­ç¬¬ä¸€ä¸ªé…ç½®æ—¥å¿—çš„åœ°æ–¹
# æ·»åŠ force=Trueå‚æ•°ç¡®ä¿è¦†ç›–å…¶ä»–æ¨¡å—çš„é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('smart_radar.log', encoding='utf-8'),
        logging.StreamHandler()
    ],
    force=True  # å¼ºåˆ¶è¦†ç›–å…¶ä»–é…ç½®
) 
logger = logging.getLogger(__name__)
VERSION = "5.1.0"

@dataclass
class Keyword:
    """å…³é”®è¯æ•°æ®ç±» - åªä¿ç•™æœ€æ ¸å¿ƒçš„å±æ€§"""
    word: str
    frequency: int
    trend_score: float
    importance: float = 0.0
    insights: Dict = field(default_factory=dict)
    
    def calculate_importance(self, weights: Optional[Dict] = None) -> None:
        """è®¡ç®—å…³é”®è¯é‡è¦æ€§ - ç®€æ´é«˜æ•ˆçš„ç®—æ³•"""
        if weights is None:
            weights = {'frequency_weight': 0.4, 'trend_weight': 0.6}
        
        self.importance = (
            self.frequency * weights['frequency_weight'] +
            self.trend_score * weights['trend_weight']
        )

class SystemState:
    """ç³»ç»ŸçŠ¶æ€ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†ç³»ç»ŸçŠ¶æ€"""
    IDLE = 'idle'
    FETCHING = 'fetching'
    ANALYZING = 'analyzing'
    LEARNING = 'learning'
    PRESENTING = 'presenting'  # æ·»åŠ å±•ç¤ºçŠ¶æ€
    COMPLETED = 'completed'    # æ·»åŠ å®ŒæˆçŠ¶æ€
    ERROR = 'error'
    
    def __init__(self):
        self.state = self.IDLE
        self.last_error = None
        self.start_time = datetime.now()
        self.metrics = {}
        
    def set_state(self, state: str, error: Optional[Exception] = None) -> None:
        self.state = state
        self.last_error = error
        
        # æ·»åŠ çŠ¶æ€å˜æ›´è¯¦æƒ…
        details = {
            'timestamp': datetime.now().isoformat(),
            'duration_seconds': (datetime.now() - self.start_time).total_seconds()
        }
        
        if error:
            error_type = type(error).__name__
            error_msg = str(error)[:100]  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
            details['error'] = f"{error_type}: {error_msg}"
            logger.error(f"ç³»ç»ŸçŠ¶æ€å˜æ›´ä¸º: {state}, é”™è¯¯: {details['error']}")
        else:
            logger.info(f"ç³»ç»ŸçŠ¶æ€å˜æ›´ä¸º: {state}")
    
    def update_metrics(self, metrics: Dict) -> None:
        """æ›´æ–°ç³»ç»Ÿè¿è¡ŒæŒ‡æ ‡"""
        self.metrics.update(metrics)
        
    def get_status_summary(self) -> Dict:
        """è·å–ç³»ç»ŸçŠ¶æ€æ‘˜è¦"""
        return {
            'state': self.state,
            'running_time': (datetime.now() - self.start_time).total_seconds(),
            'metrics': self.metrics,
            'last_error': self.last_error is not None
        }

class KeywordAnalyzer:
    """æ™ºèƒ½å…³é”®è¯åˆ†æå™¨ - ä¸“æ³¨äºé«˜è´¨é‡å…³é”®è¯æå–"""
    
    def __init__(self):
        self.stopwords = self._load_stopwords()
        self._setup_jieba()
        
    def _setup_jieba(self) -> None:
        """é…ç½®jiebaåˆ†è¯å™¨ä»¥è·å¾—æœ€ä½³æ€§èƒ½"""
        import platform
        if platform.system() != 'Windows':
            try:
                jieba.enable_parallel(4)  # åªåœ¨éWindowsç³»ç»Ÿä¸Šå¼€å¯å¹¶è¡Œåˆ†è¯
            except Exception as e:
                logger.warning(f"å¯ç”¨å¹¶è¡Œåˆ†è¯å¤±è´¥: {e}")
    
    def _load_stopwords(self) -> Set[str]:
        """åŠ è½½ä¼˜åŒ–åçš„åœç”¨è¯è¡¨ - ä¸“æ³¨äºä¸­æ–‡æ–°é—»æ–‡æœ¬"""
        return {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä¸º', 'ä¸', 'æˆ–', 'ä»¥åŠ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ',
            'ç½‘å‹', 'ç”¨æˆ·', 'è¡¨ç¤º', 'è®¤ä¸º', 'ç§°', 'æ®', 'æ˜¾ç¤º', 'æŠ¥é“', 'æ¶ˆæ¯', 'è®°è€…',
            'ä»Šæ—¥', 'æ˜¨æ—¥', 'æ˜æ—¥', 'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'ä¸Šåˆ', 'ä¸‹åˆ', 'æ™šä¸Š',
            'æœ€æ–°', 'çƒ­é—¨', 'çƒ­æœ', 'æ’è¡Œ', 'æ¦œå•', 'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰'
        }
    
    async def extract_keywords(self, news_items: List, max_keywords: int = 20) -> List[Keyword]:
        """å¼‚æ­¥ä»æ–°é—»ä¸­æå–é«˜è´¨é‡å…³é”®è¯"""
        if not news_items:
            return []
        
        # æå–æ‰€æœ‰æ ‡é¢˜æ–‡æœ¬
        all_titles = [item.title for item in news_items]
        
        # ä½¿ç”¨jiebaçš„TF-IDFæå–å…³é”®è¯
        combined_text = ' '.join(all_titles)
        keywords_with_weight = jieba.analyse.extract_tags(
            combined_text, 
            topK=max_keywords * 2,  # æå–æ›´å¤šå…³é”®è¯ä»¥è¿‡æ»¤
            withWeight=True, 
            allowPOS=('n', 'nr', 'ns', 'nt', 'nz')  # åªæå–åè¯ç›¸å…³è¯æ€§
        )
        
        # ç»Ÿè®¡è¯é¢‘
        all_words = []
        for title in all_titles:
            words = self._segment_text(title)
            all_words.extend(words)
        word_freq = Counter(all_words)
        
        # å¤„ç†å…³é”®è¯è®¡ç®—
        keywords = []
        for word, tfidf_weight in keywords_with_weight:
            if word in self.stopwords or len(word) < 2:
                continue
            
            freq = word_freq.get(word, 0)
            if freq < 2:  # è¿‡æ»¤ä½é¢‘è¯
                continue
            
            trend_score = self._calculate_trend_score(word, news_items)
            
            keyword = Keyword(
                word=word,
                frequency=freq,
                trend_score=trend_score
            )
            keyword.calculate_importance()
            keywords.append(keyword)
        
        # æŒ‰é‡è¦æ€§æ’åº
        keywords.sort(key=lambda x: x.importance, reverse=True)
        
        return keywords[:max_keywords]
    
    def _segment_text(self, text: str) -> List[str]:
        """ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡æ–‡æœ¬åˆ†è¯ - ç²¾ç®€é«˜æ•ˆ"""
        # ç§»é™¤æ•°å­—ã€ç¬¦å·ç­‰
        text = re.sub(r'[^ä¸€-é¾¥a-zA-Z]+', ' ', text)
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        return [word for word in words if word not in self.stopwords and len(word) >= 2]
    
    def _calculate_trend_score(self, word: str, news_items: List) -> float:
        """è®¡ç®—å…³é”®è¯è¶‹åŠ¿åˆ†æ•° - åŸºäºæ–°é—»æ’åçš„ç®€æ´ç®—æ³•"""
        matching_items = [item for item in news_items if word in item.title]
        if not matching_items:
            return 0.0
        
        avg_rank = sum(max(0, 20 - item.rank) for item in matching_items) / len(matching_items)
        return min(1.0, avg_rank / 20.0)

class SmartNewsRadar:
    """æ™ºèƒ½æ–°é—»é›·è¾¾ä¸»ç±» - ç®€æ´ä¼˜é›…çš„åè°ƒè€…"""
    
    def __init__(self, config_file: str = 'smart_config.yaml'):
        self.config = self._load_config(config_file)
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.news_fetcher = NewsFetcher("news_sources.json")
        self.analyzer = KeywordAnalyzer()
        self.learning_engine = SmartLearningEngine(self.config.get('learning', {}))
        self.state_manager = SystemState()
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not Path(config_file).exists():
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            return {'core': {'max_keywords': 50}}
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f) or {'core': {'max_keywords': 50}}
        except Exception as e:
            error_type = type(e).__name__
            logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {error_type}")
            return {'core': {'max_keywords': 50}}
    
    async def run(self):
        """å¼‚æ­¥è¿è¡Œä¸»ç¨‹åº - ä¼˜é›…é«˜æ•ˆçš„æµç¨‹æ§åˆ¶"""
        logger.info(f"SmartNewsRadar v{VERSION} å¯åŠ¨")
        
        try:
            # è·å–æ–°é—»æ•°æ®
            logger.info("å¼€å§‹è·å–æ–°é—»æ•°æ®...")
            self.state_manager.set_state(SystemState.FETCHING)
            news_items = await self._safe_fetch_news()
            
            logger.info(f"å…±è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            self.state_manager.update_metrics({'news_count': len(news_items)})
            
            if not news_items:
                logger.warning("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
                return
            
            # åˆ†æå…³é”®è¯
            logger.info("å¼€å§‹åˆ†æå…³é”®è¯...")
            self.state_manager.set_state(SystemState.ANALYZING)
            max_keywords = self.config.get('core', {}).get('max_keywords', 50)
            keywords = await self.analyzer.extract_keywords(news_items, max_keywords)
            
            logger.info(f"è‡ªåŠ¨å‘ç° {len(keywords)} ä¸ªå…³é”®è¯")
            self.state_manager.update_metrics({'keywords_count': len(keywords)})
            
            # è°ƒç”¨å­¦ä¹ å¼•æ“
            logger.info("å¼€å§‹è‡ªé€‚åº”å­¦ä¹ ...")
            self.state_manager.set_state(SystemState.LEARNING)
            self.learning_engine.learn_from_keywords(keywords, news_items)
            
            # æ›´æ–°å…³é”®è¯æ´å¯Ÿ
            self._enrich_keywords_with_insights(keywords)
            
            # æ„å»ºæ–°é—»æ‘˜è¦
            news_summary = []
            for item in news_items[:10]:  # å‰10æ¡æ–°é—»
                news_summary.append({
                    'title': item.title,
                    'source': item.source,
                    'rank': item.rank if hasattr(item, 'rank') else 0
                })
            
            # è·å–å­¦ä¹ ç»Ÿè®¡æ•°æ®
            learning_stats = {
                'keywords_count': len(self.learning_engine._learning_data.get('keywords', {})),
                'weights_adjusted': len(self.learning_engine._learning_data.get('weight_history', [])),
                'current_weights': self.learning_engine.optimal_weights
            }
            
            # æ„å»ºå®Œæ•´çš„åˆ†æç»“æœæ•°æ®
            analysis_result = {
                'timestamp': datetime.now().isoformat(),
                'total_news': len(news_items),
                'keywords': [kw.__dict__ for kw in keywords],  # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                'news_summary': news_summary,
                'learning_stats': learning_stats  # æ·»åŠ å­¦ä¹ ç»Ÿè®¡æ•°æ®
            }
            
            # ä½¿ç”¨æ–°çš„å±•ç¤ºå±‚å‘ˆç°ç»“æœ
            logger.info("å¼€å§‹ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            self.state_manager.set_state(SystemState.PRESENTING)
            
            # é¦–å…ˆç”ŸæˆHTMLæŠ¥å‘Š
            report_path = present_results(analysis_result, "html", self.config)
            # ç§»é™¤ä¸‹é¢è¿™è¡Œé‡å¤çš„æ—¥å¿—
            # logger.info(f"HTMLåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
            
            # æ£€æŸ¥é…ç½®æ˜¯å¦éœ€è¦ç”ŸæˆJSONæŠ¥å‘Š
            if self.config.get('presentation', {}).get('generate_json', True):
                json_path = present_results(analysis_result, "json", self.config)
                # åŒæ ·ç§»é™¤è¿™è¡Œé‡å¤çš„æ—¥å¿—
                # logger.info(f"JSONåˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {json_path}")
            
            # ç„¶åä½¿ç”¨æ§åˆ¶å°æ ¼å¼è¾“å‡ºï¼Œç¡®ä¿ç”¨æˆ·èƒ½åœ¨å‘½ä»¤è¡Œçœ‹åˆ°ç»“æœ
            console_output = present_results(analysis_result, "console", self.config)
            
            self.state_manager.set_state(SystemState.COMPLETED)
            
            # æ·»åŠ HTMLæŠ¥å‘Šåœ°å€æ˜¾ç¤ºå’Œè‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨åŠŸèƒ½
            if report_path:
                print(f"\nâœ¨ åˆ†æå·²å®Œæˆï¼HTMLæŠ¥å‘Šå·²ç”Ÿæˆ:\nğŸ“ {report_path}")
                # è¯¢é—®ç”¨æˆ·æ˜¯å¦è¦æ‰“å¼€æµè§ˆå™¨æŸ¥çœ‹æŠ¥å‘Š
                try:
                    import webbrowser
                    import os
                    # è·å–ç»å¯¹è·¯å¾„å¹¶è½¬æ¢ä¸ºURLæ ¼å¼
                    abs_path = os.path.abspath(report_path)
                    # å¯¹äºWindowsç³»ç»Ÿï¼Œéœ€è¦ç‰¹åˆ«å¤„ç†è·¯å¾„æ ¼å¼
                    if os.name == 'nt':  # Windowsç³»ç»Ÿ
                        url = f"file:///{abs_path.replace('\\', '/')}"
                    else:  # å…¶ä»–ç³»ç»Ÿ
                        url = f"file://{abs_path}"
                    
                    # è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨
                    webbrowser.open(url)
                    print(f"ğŸŒ å·²å°è¯•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨æŸ¥çœ‹æŠ¥å‘Š")
                except Exception as e:
                    print(f"âš ï¸ æ— æ³•è‡ªåŠ¨æ‰“å¼€æµè§ˆå™¨: {str(e)}")
                    print(f"ğŸ“Œ è¯·æ‰‹åŠ¨æ‰“å¼€æ–‡ä»¶: {report_path}")
            
            return keywords
            
        except Exception as e:
            self.state_manager.set_state(SystemState.ERROR, e)
            logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}", exc_info=True)
            return []
    
    async def _safe_fetch_news(self):
        """å®‰å…¨è·å–æ–°é—»æ•°æ®ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        max_retries = 2
        retry_count = 0
        
        while retry_count <= max_retries:
            try:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self.news_fetcher.fetch)
            except Exception as e:
                retry_count += 1
                if retry_count > max_retries:
                    raise
                error_type = type(e).__name__
                logger.warning(f"è·å–æ–°é—»æ•°æ®å¤±è´¥({error_type})ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})...")
                await asyncio.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
    
    def _enrich_keywords_with_insights(self, keywords: List[Keyword]) -> None:
        """ä¸ºå…³é”®è¯æ·»åŠ å­¦ä¹ å¼•æ“æä¾›çš„æ´å¯Ÿ"""
        for keyword in keywords[:10]:  # å–æ’åé å‰çš„å…³é”®è¯
            insights = self.learning_engine.get_keyword_insights(keyword.word)
            if insights:
                keyword.insights = {
                    'trend': insights.get('trend_direction', 'unknown'),
                    'appearances': insights.get('appearances', 0),
                    'first_seen': insights.get('first_seen', 'unknown')
                }
    
    def start(self):
        """å¯åŠ¨ç¨‹åºå…¥å£"""
        try:
            asyncio.run(self.run())
        except RuntimeError:
            # å¤„ç†äº‹ä»¶å¾ªç¯å·²å­˜åœ¨çš„æƒ…å†µ
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(self.run())
            finally:
                loop.close()

    # ä¿®å¤process_newsæ–¹æ³•
    async def process_news(self) -> List[Dict]:
        """å¤„ç†æ–°é—»æ•°æ®ï¼Œæå–å…³é”®è¯å¹¶è¿›è¡Œåˆ†æ"""
        try:
            # ç›´æ¥è°ƒç”¨runæ–¹æ³•è·å–å…³é”®è¯
            keywords = await self.run()
            return [kw.__dict__ for kw in keywords] if keywords else []
        except Exception as e:
            logger.error(f"å¤„ç†æ–°é—»æ•°æ®å‡ºé”™: {e}")
            return []

if __name__ == "__main__":
    radar = SmartNewsRadar()
    radar.start()