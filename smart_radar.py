# -*- coding: utf-8 -*-
"""
SmartNewsRadar - æ™ºèƒ½æ–°é—»å…³é”®è¯å‘ç°ç³»ç»Ÿ
åŸºäºAIé©±åŠ¨çš„çœŸå®ä¸–ç•Œçƒ­ç‚¹å…³é”®è¯è‡ªåŠ¨å‘ç°

Author: Inspired by Jobs' philosophy of simplicity
Version: 1.0.0
License: MIT
"""

import json
import os
import re
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Set
from collections import Counter, defaultdict
import math
import logging
import asyncio

import requests
import yaml
from dataclasses import dataclass

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from smart_learning import AdaptiveLearningEngine
from smart_notifier import SmartNotifier
# ä½¿ç”¨æ›´é«˜çº§çš„åˆ†è¯åº“
import jieba
import jieba.analyse

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('smart_radar.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

VERSION = "1.0.0"

@dataclass
class NewsItem:
    """æ–°é—»é¡¹æ•°æ®ç±»"""
    title: str
    source: str
    rank: int
    url: str = ""
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()

@dataclass
class Keyword:
    """å…³é”®è¯æ•°æ®ç±»"""
    word: str
    frequency: int
    trend_score: float
    sentiment_score: float
    importance: float = 0.0
    
    def calculate_importance(self, weights=None):
        """è®¡ç®—å…³é”®è¯é‡è¦æ€§"""
        if weights is None:
            weights = {
                'frequency_weight': 0.4,
                'trend_weight': 0.4,
                'sentiment_weight': 0.2
            }
        
        self.importance = (
            self.frequency * weights['frequency_weight'] +
            self.trend_score * weights['trend_weight'] +
            abs(self.sentiment_score) * weights['sentiment_weight']
        )

class SmartKeywordAnalyzer:
    """æ™ºèƒ½å…³é”®è¯åˆ†æå™¨"""
    
    def __init__(self, learning_engine=None):
        self.stopwords = self._load_stopwords()
        self.keyword_history = defaultdict(list)
        self.learning_engine = learning_engine  # è‡ªé€‚åº”å­¦ä¹ å¼•æ“
        
        # é…ç½®jiebaåˆ†è¯
        self._setup_jieba()
        
    def _setup_jieba(self):
        """é…ç½®jiebaåˆ†è¯å™¨"""
        # æ·»åŠ è‡ªå®šä¹‰è¯å…¸ï¼ˆå¦‚æœæœ‰ï¼‰
        # jieba.load_userdict('custom_dict.txt')
        
        # å¯¼å…¥platformæ¨¡å—æ£€æµ‹æ“ä½œç³»ç»Ÿ
        import platform
        if platform.system() != 'Windows':
            # åªåœ¨éWindowsç³»ç»Ÿä¸Šå¼€å¯å¹¶è¡Œåˆ†è¯
            try:
                jieba.enable_parallel(4)  # ä½¿ç”¨4ä¸ªè¿›ç¨‹
            except Exception as e:
                logger.warning(f"å¯ç”¨å¹¶è¡Œåˆ†è¯å¤±è´¥: {e}")
        
        # ç§»é™¤set_idf_path(None)è°ƒç”¨ï¼Œç›´æ¥ä½¿ç”¨é»˜è®¤çš„TF-IDFé…ç½®
        # jiebaä¼šè‡ªåŠ¨ä½¿ç”¨å†…ç½®çš„idfæƒé‡ï¼Œä¸éœ€è¦é¢å¤–é…ç½®
        
    def _load_stopwords(self) -> Set[str]:
        """åŠ è½½åœç”¨è¯è¡¨"""
        default_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
            'è‡ªå·±', 'è¿™', 'é‚£', 'ä¸º', 'ä¸', 'æˆ–', 'ä»¥åŠ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ',
            'ç½‘å‹', 'ç”¨æˆ·', 'è¡¨ç¤º', 'è®¤ä¸º', 'ç§°', 'æ®', 'æ˜¾ç¤º', 'æŠ¥é“', 'æ¶ˆæ¯', 'è®°è€…',
            'ä»Šæ—¥', 'æ˜¨æ—¥', 'æ˜æ—¥', 'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©', 'ä¸Šåˆ', 'ä¸‹åˆ', 'æ™šä¸Š',
            'æœ€æ–°', 'çƒ­é—¨', 'çƒ­æœ', 'æ’è¡Œ', 'æ¦œå•', 'ç¬¬ä¸€', 'ç¬¬äºŒ', 'ç¬¬ä¸‰'
        }
        return default_stopwords
    
    def extract_keywords(self, news_items: List[NewsItem], max_keywords: int = 20) -> List[Keyword]:
        """ä»æ–°é—»ä¸­æå–å…³é”®è¯"""
        # æå–æ‰€æœ‰æ ‡é¢˜æ–‡æœ¬
        all_titles = [item.title for item in news_items]
        
        # ä½¿ç”¨jiebaçš„TF-IDFæå–å…³é”®è¯
        # é¦–å…ˆåˆå¹¶æ‰€æœ‰æ ‡é¢˜
        combined_text = ' '.join(all_titles)
        
        # æå–å…³é”®è¯åŠå…¶æƒé‡
        keywords_with_weight = jieba.analyse.extract_tags(
            combined_text, 
            topK=max_keywords * 2,  # æå–æ›´å¤šå…³é”®è¯ä»¥è¿‡æ»¤
            withWeight=True, 
            allowPOS=('n', 'nr', 'ns', 'nt', 'nz')  # åªæå–åè¯ç›¸å…³è¯æ€§
        )
        
        # ç»Ÿè®¡è¯é¢‘
        all_words = []
        for title in all_titles:
            words = self._segment_text(title)
            all_words.extend(words)
        word_freq = Counter(all_words)
        
        # å¦‚æœæœ‰å­¦ä¹ å¼•æ“ï¼Œè·å–å½“å‰æœ€ä¼˜æƒé‡
        weights = None
        if self.learning_engine:
            weights = self.learning_engine.optimal_weights
            logger.info(f"ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ æƒé‡: {weights}")
        
        keywords = []
        for word, tfidf_weight in keywords_with_weight:
            if word in self.stopwords or len(word) < 2:
                continue
            
            # è·å–è¯é¢‘
            freq = word_freq.get(word, 0)
            if freq < 2:  # è¿‡æ»¤ä½é¢‘è¯
                continue
            
            trend_score = self._calculate_trend_score(word, news_items)
            sentiment_score = self._analyze_sentiment(word, news_items)
            
            keyword = Keyword(
                word=word,
                frequency=freq,
                trend_score=trend_score,
                sentiment_score=sentiment_score
            )
            keyword.calculate_importance(weights)
            keywords.append(keyword)
        
        # æŒ‰é‡è¦æ€§æ’åº
        keywords.sort(key=lambda x: x.importance, reverse=True)
        
        # æ›´æ–°å†å²è®°å½•
        self._update_keyword_history(keywords)
        
        # å¦‚æœæœ‰å­¦ä¹ å¼•æ“ï¼Œä»ç»“æœä¸­å­¦ä¹ 
        if self.learning_engine:
            selected_keywords = keywords[:max_keywords]
            updated_weights = self.learning_engine.learn_from_keywords(selected_keywords, news_items)
            logger.info(f"å­¦ä¹ å¼•æ“åˆ†æå®Œæˆï¼Œæ›´æ–°æƒé‡: {updated_weights}")
        
        return keywords[:max_keywords]
    
    def _segment_text(self, text: str) -> List[str]:
        """ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡æ–‡æœ¬åˆ†è¯"""
        # ç§»é™¤æ•°å­—ã€ç¬¦å·ç­‰
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z]+', ' ', text)
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text)
        
        # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
        filtered_words = [
            word for word in words 
            if word not in self.stopwords and len(word) >= 2
        ]
        
        return filtered_words
    
    def _calculate_trend_score(self, word: str, news_items: List[NewsItem]) -> float:
        """è®¡ç®—å…³é”®è¯è¶‹åŠ¿åˆ†æ•°"""
        # åŸºäºæ–°é—»æ’åå’Œæ—¶é—´çš„è¶‹åŠ¿åˆ†æ
        rank_scores = []
        for item in news_items:
            if word in item.title:
                # æ’åè¶Šé«˜åˆ†æ•°è¶Šé«˜
                rank_score = max(0, 20 - item.rank)
                rank_scores.append(rank_score)
        
        if not rank_scores:
            return 0.0
        
        # è®¡ç®—å¹³å‡æ’ååˆ†æ•°
        avg_rank_score = sum(rank_scores) / len(rank_scores)
        
        # å½’ä¸€åŒ–åˆ°0-1
        return min(1.0, avg_rank_score / 20.0)
    
    def _analyze_sentiment(self, word: str, news_items: List[NewsItem]) -> float:
        """ç®€å•çš„æƒ…æ„Ÿåˆ†æ"""
        positive_indicators = ['æˆåŠŸ', 'çªç ´', 'å¢é•¿', 'ä¸Šæ¶¨', 'èƒœåˆ©', 'å¥½', 'ä¼˜', 'èµ', 'æ£’']
        negative_indicators = ['å¤±è´¥', 'ä¸‹è·Œ', 'å±æœº', 'é—®é¢˜', 'é”™è¯¯', 'å·®', 'å', 'æ‰¹è¯„']
        
        sentiment_score = 0.0
        count = 0
        
        for item in news_items:
            if word in item.title:
                title_lower = item.title.lower()
                for pos in positive_indicators:
                    if pos in title_lower:
                        sentiment_score += 1
                for neg in negative_indicators:
                    if neg in title_lower:
                        sentiment_score -= 1
                count += 1
        
        return sentiment_score / max(1, count)
    
    def _update_keyword_history(self, keywords: List[Keyword]):
        """æ›´æ–°å…³é”®è¯å†å²è®°å½•"""
        current_time = datetime.now()
        for keyword in keywords:
            self.keyword_history[keyword.word].append({
                'timestamp': current_time,
                'frequency': keyword.frequency,
                'importance': keyword.importance
            })
            
            # åªä¿ç•™æœ€è¿‘7å¤©çš„è®°å½•
            self.keyword_history[keyword.word] = [
                record for record in self.keyword_history[keyword.word]
                if current_time - record['timestamp'] <= timedelta(days=7)
            ]

class SmartReportGenerator:
    """æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict):
        self.config = config
    
    def generate_html_report(self, keywords: List[Keyword], news_items: List[NewsItem]) -> str:
        """ç”ŸæˆHTMLæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ™ºèƒ½æ–°é—»é›·è¾¾ - è‡ªåŠ¨å‘ç°çš„çƒ­ç‚¹å…³é”®è¯</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}
        
        body {{
            font-family: 'SF Pro Display', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            padding: 20px;
            color: #333;
        }}
        
        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            overflow: hidden;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
        }}
        
        .header {{
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            color: white;
            padding: 40px 30px;
            text-align: center;
        }}
        
        .header h1 {{
            font-size: 2.5rem;
            font-weight: 300;
            margin-bottom: 10px;
        }}
        
        .header .subtitle {{
            font-size: 1.1rem;
            opacity: 0.9;
        }}
        
        .content {{
            padding: 40px 30px;
        }}
        
        .keywords-section {{
            margin-bottom: 50px;
        }}
        
        .section-title {{
            font-size: 1.8rem;
            font-weight: 600;
            margin-bottom: 30px;
            color: #1e3c72;
        }}
        
        .keywords-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}
        
        .keyword-card {{
            background: #f8f9ff;
            padding: 20px;
            border-radius: 15px;
            border-left: 4px solid #2a5298;
            transition: transform 0.3s ease;
        }}
        
        .keyword-card:hover {{
            transform: translateY(-5px);
        }}
        
        .keyword-title {{
            font-size: 1.3rem;
            font-weight: 600;
            margin-bottom: 10px;
            color: #1e3c72;
        }}
        
        .keyword-stats {{
            display: flex;
            justify-content: space-between;
            margin-bottom: 15px;
        }}
        
        .stat {{
            text-align: center;
        }}
        
        .stat-value {{
            font-size: 1.1rem;
            font-weight: 600;
            color: #2a5298;
        }}
        
        .stat-label {{
            font-size: 0.8rem;
            color: #666;
            margin-top: 5px;
        }}
        
        .importance-bar {{
            height: 6px;
            background: #e0e0e0;
            border-radius: 3px;
            overflow: hidden;
        }}
        
        .importance-fill {{
            height: 100%;
            background: linear-gradient(90deg, #2a5298, #667eea);
            transition: width 0.5s ease;
        }}
        
        .related-news {{
            margin-top: 15px;
        }}
        
        .news-item {{
            background: white;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 10px;
            border: 1px solid #e0e0e0;
        }}
        
        .news-source {{
            font-size: 0.8rem;
            color: #666;
            margin-bottom: 5px;
        }}
        
        .news-title {{
            font-size: 0.95rem;
            line-height: 1.4;
        }}
        
        .timestamp {{
            text-align: center;
            margin-top: 40px;
            padding: 20px;
            background: #f8f9ff;
            border-radius: 10px;
            color: #666;
        }}
        
        @media (max-width: 768px) {{
            .keywords-grid {{
                grid-template-columns: 1fr;
            }}
            
            .header h1 {{
                font-size: 2rem;
            }}
            
            .content {{
                padding: 20px;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¯ æ™ºèƒ½æ–°é—»é›·è¾¾</h1>
            <div class="subtitle">AIé©±åŠ¨çš„çƒ­ç‚¹å…³é”®è¯è‡ªåŠ¨å‘ç°ç³»ç»Ÿ</div>
        </div>
        
        <div class="content">
            <div class="keywords-section">
                <h2 class="section-title">ğŸ”¥ è‡ªåŠ¨å‘ç°çš„çƒ­ç‚¹å…³é”®è¯</h2>
                <div class="keywords-grid">
                    {self._generate_keyword_cards(keywords, news_items)}
                </div>
            </div>
            
            <div class="timestamp">
                <div>æ•°æ®æ›´æ–°æ—¶é—´: {timestamp}</div>
                <div style="margin-top: 10px; font-size: 0.9rem;">
                    å…±åˆ†æäº† {len(news_items)} æ¡æ–°é—»ï¼Œè‡ªåŠ¨å‘ç° {len(keywords)} ä¸ªçƒ­ç‚¹å…³é”®è¯
                </div>
            </div>
        </div>
    </div>
</body>
</html>
        """
        
        return html_content
    
    def _generate_keyword_cards(self, keywords: List[Keyword], news_items: List[NewsItem]) -> str:
        """ç”Ÿæˆå…³é”®è¯å¡ç‰‡HTML"""
        cards_html = ""
        
        for keyword in keywords:
            # æ‰¾åˆ°åŒ…å«è¯¥å…³é”®è¯çš„ç›¸å…³æ–°é—»
            related_news = [
                item for item in news_items
                if keyword.word in item.title
            ][:3]  # åªæ˜¾ç¤ºå‰3æ¡ç›¸å…³æ–°é—»
            
            related_news_html = ""
            for news in related_news:
                related_news_html += f"""
                <div class="news-item">
                    <div class="news-source">[{news.source}] æ’å #{news.rank}</div>
                    <div class="news-title">{news.title}</div>
                </div>
                """
            
            cards_html += f"""
            <div class="keyword-card">
                <div class="keyword-title">{keyword.word}</div>
                <div class="keyword-stats">
                    <div class="stat">
                        <div class="stat-value">{keyword.frequency}</div>
                        <div class="stat-label">å‡ºç°æ¬¡æ•°</div>
                    </div>
                    <div class="stat">
                        <div class="stat-value">{keyword.trend_score:.2f}</div>
                        <div class="stat-label">è¶‹åŠ¿æŒ‡æ•°</div>
                    </div>
                    <div class="stat">
                        <div class="stat-value">{keyword.importance:.1f}</div>
                        <div class="stat-label">é‡è¦æ€§</div>
                    </div>
                </div>
                <div class="importance-bar">
                    <div class="importance-fill" style="width: {min(100, keyword.importance * 10)}%"></div>
                </div>
                <div class="related-news">
                    {related_news_html}
                </div>
            </div>
            """
        
        return cards_html
    
    def generate_json_report(self, keywords: List[Keyword], news_items: List[NewsItem]) -> Dict:
        """ç”ŸæˆJSONæ ¼å¼æŠ¥å‘Š"""
        return {
            'timestamp': datetime.now().isoformat(),
            'total_news': len(news_items),
            'total_keywords': len(keywords),
            'keywords': [
                {
                    'word': kw.word,
                    'frequency': kw.frequency,
                    'trend_score': kw.trend_score,
                    'sentiment_score': kw.sentiment_score,
                    'importance': kw.importance
                }
                for kw in keywords
            ],
            'news_summary': [
                {
                    'title': item.title,
                    'source': item.source,
                    'rank': item.rank
                }
                for item in news_items[:20]  # åªä¿å­˜å‰20æ¡æ–°é—»
            ]
        }

class SmartNewsRadar:
    """æ™ºèƒ½æ–°é—»é›·è¾¾ä¸»ç±»"""
    
    def __init__(self, config_file: str = 'smart_config.yaml'):
        self.config = self._load_config(config_file)
        
        # åˆå§‹åŒ–å­¦ä¹ å¼•æ“
        self.learning_engine = AdaptiveLearningEngine(self.config)
        
        # åˆå§‹åŒ–ç»„ä»¶ - ä½¿ç”¨å¢å¼ºç‰ˆæ•°æ®è·å–å™¨
        from enhanced_data_fetcher import EnhancedDataFetcher as RealEnhancedDataFetcher
        self.fetcher = RealEnhancedDataFetcher(self.config)
        self.analyzer = SmartKeywordAnalyzer(self.learning_engine)
        self.reporter = SmartReportGenerator(self.config)
        self.notifier = SmartNotifier(self.config)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path('output').mkdir(exist_ok=True)
    
    def _load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not Path(config_file).exists():
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            return self._create_default_config()
        
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def _create_default_config(self) -> Dict:
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        return {
            'data_sources': {
                'enhanced_mode': True,
                'timeout': 30,
                'concurrent_limit': 20,
                'newsnow_api': {
                    'enabled': True,
                    'sources': [
                        {'id': 'toutiao', 'name': 'ä»Šæ—¥å¤´æ¡'},
                        {'id': 'baidu', 'name': 'ç™¾åº¦çƒ­æœ'},
                        {'id': 'weibo', 'name': 'å¾®åšçƒ­æœ'},
                        {'id': 'zhihu', 'name': 'çŸ¥ä¹çƒ­æ¦œ'},
                        {'id': 'douyin', 'name': 'æŠ–éŸ³çƒ­æœ'},
                        {'id': 'bilibili-hot-search', 'name': 'Bç«™çƒ­æœ'}
                    ]
                },
                'rss_feeds': {
                    'enabled': True,
                    'sources': []
                },
                'web_scraping': {
                    'enabled': False,
                    'sources': []
                }
            },
            'ai_analysis': {
                'max_keywords': 50,
                'weights': {
                    'frequency_weight': 0.4,
                    'trend_weight': 0.4,
                    'sentiment_weight': 0.2
                }
            },
            'notification': {
                'enabled': False,
                'webhooks': {}
            }
        }
    
    def run(self):
        """è¿è¡Œä¸»ç¨‹åº"""
        logger.info(f"SmartNewsRadar v{VERSION} å¯åŠ¨")
        
        try:
            # è·å–æ–°é—»æ•°æ®
            logger.info("å¼€å§‹è·å–æ–°é—»æ•°æ®...")
            
            # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•è·å–æ‰€æœ‰æ•°æ®æº
            news_data = asyncio.run(self.fetcher.fetch_all_news_async())
            
            # è½¬æ¢ä¸ºNewsItemå¯¹è±¡
            news_items = []
            for item in news_data:
                news_item = NewsItem(
                    title=item['title'],
                    source=item['source'],
                    rank=item['rank'],
                    url=item.get('url', ''),
                    timestamp=datetime.fromisoformat(item['timestamp'])
                )
                news_items.append(news_item)
            
            logger.info(f"å…±è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            
            if not news_items:
                logger.warning("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
                return
            
            # åˆ†æå…³é”®è¯
            logger.info("å¼€å§‹åˆ†æå…³é”®è¯...")
            max_keywords = self.config.get('ai_analysis', {}).get('max_keywords', 50)
            keywords = self.analyzer.extract_keywords(news_items, max_keywords)
            
            logger.info(f"è‡ªåŠ¨å‘ç° {len(keywords)} ä¸ªå…³é”®è¯")
            
            # ç”ŸæˆæŠ¥å‘Š
            logger.info("ç”ŸæˆæŠ¥å‘Š...")
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            
            # ç”ŸæˆHTMLæŠ¥å‘Š
            html_report = self.reporter.generate_html_report(keywords, news_items)
            html_file = f"output/smart_radar_{timestamp}.html"
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(html_report)
            
            # ç”ŸæˆJSONæŠ¥å‘Š
            json_report = self.reporter.generate_json_report(keywords, news_items)
            json_file = f"output/smart_radar_{timestamp}.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(json_report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {html_file}, {json_file}")
            
            # å‘é€é€šçŸ¥
            if self.config.get('notification', {}).get('enabled', False):
                self.notifier.send_notification(keywords[:10])  # åªå‘é€å‰10ä¸ªå…³é”®è¯
            else:
                logger.info("é€šçŸ¥åŠŸèƒ½æœªå¯ç”¨")
            
            # æ˜¾ç¤ºç»“æœåˆ°æ§åˆ¶å°
            self._display_results(keywords)
            
        except Exception as e:
            logger.error(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}", exc_info=True)
    
    def _display_results(self, keywords: List[Keyword]):
        """åœ¨æ§åˆ¶å°æ˜¾ç¤ºç»“æœ"""
        print("\nğŸ¯ è‡ªåŠ¨å‘ç°çš„çƒ­ç‚¹å…³é”®è¯:")
        print("============================================================")
        
        # æ˜¾ç¤ºå‰10ä¸ªå…³é”®è¯
        for i, keyword in enumerate(keywords[:10], 1):
            print(f"{i:2d}. {keyword.word:<10} | é‡è¦æ€§: {keyword.importance:5.2f} | å‡ºç°: {keyword.frequency:2d}æ¬¡ | è¶‹åŠ¿: {keyword.trend_score:.2f}")
        
        print("\nğŸ¤– AIå­¦ä¹ ç»Ÿè®¡:")
        print(f"  â€¢ å­¦ä¹ å…³é”®è¯æ•°: {len(keywords)}")
        
        if hasattr(self.analyzer, 'learning_engine') and self.analyzer.learning_engine:
            # ä½¿ç”¨weight_historyçš„é•¿åº¦ä½œä¸ºæƒé‡è°ƒæ•´æ¬¡æ•°
            print(f"  â€¢ æƒé‡è°ƒæ•´æ¬¡æ•°: {len(self.analyzer.learning_engine.weight_history)}")
            print(f"  â€¢ å½“å‰æœ€ä¼˜æƒé‡:")
            for key, value in self.analyzer.learning_engine.optimal_weights.items():
                print(f"    - {key}: {value:.3f}")

if __name__ == "__main__":
    radar = SmartNewsRadar()
    radar.run()